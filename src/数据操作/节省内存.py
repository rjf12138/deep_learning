import torch

'''
运⾏⼀些操作可能会导致为新结果分配内存。例如，如果我们⽤Y = X + Y，我们将取消引⽤Y指向的张量，
⽽是指向新分配的内存处的张量。
在下⾯的例⼦中，我们⽤Python的id()函数演⽰了这⼀点，它给我们提供了内存中引⽤对象的确切地址。运
⾏Y = Y + X后，我们会发现id(Y)指向另⼀个位置。这是因为Python⾸先计算Y + X，为结果分配新的内存，
然后使Y指向内存中的这个新位置。
'''

X=torch.arange(12)
Y=torch.arange(12)

before = id(Y)
Y = Y + X
print("id(Y) == before: ", id(Y) == before)

'''
这可能是不可取的，原因有两个：
1. ⾸先，我们不想总是不必要地分配内存。在机器学习中，我们可能有数百兆的参数，并且在⼀秒内多次
更新所有参数。通常情况下，我们希望原地执⾏这些更新；
2. 如果我们不原地更新，其他引⽤仍然会指向旧的内存位置，这样我们的某些代码可能会⽆意中引⽤旧
的参数。
幸运的是，执⾏原地操作⾮常简单。我们可以使⽤切⽚表⽰法将操作的结果分配给先前分配的数组，例如Y[:]
= <expression>。为了说明这⼀点，我们⾸先创建⼀个新的矩阵Z，其形状与另⼀个Y相同，使⽤zeros_like来
分配⼀个全0的块。
'''
Z = torch.zeros_like(Y)
print('id(Z):', id(Z))
Z[:] = X + Y
print('id(Z):', id(Z))

# 如果在后续计算中没有重复使⽤X，我们也可以使⽤X[:] = X + Y或X += Y来减少操作的内存开销。
before = id(X)
X += Y
print("id(X) == before: ", id(X) == before)